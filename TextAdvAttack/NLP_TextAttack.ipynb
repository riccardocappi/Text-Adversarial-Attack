{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc9BVpsSmZBC"
      },
      "source": [
        "## Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUDjgWcpx3TW"
      },
      "outputs": [],
      "source": [
        "!pip install textattack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XFVPlsYMWRJ"
      },
      "outputs": [],
      "source": [
        "import textattack\n",
        "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
        "import transformers\n",
        "from textattack.attack_results.successful_attack_result import SuccessfulAttackResult\n",
        "from textattack.shared.word_embeddings import WordEmbedding\n",
        "import numpy as np\n",
        "import json\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "from IPython.core.display import display, HTML\n",
        "from textattack.loggers import CSVLogger\n",
        "import torch\n",
        "from textattack.models.helpers.lstm_for_classification import LSTMForClassification\n",
        "from textattack.models.wrappers.pytorch_model_wrapper import PyTorchModelWrapper\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from textattack.models.helpers.word_cnn_for_classification import WordCNNForClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzSRnUNspUz4"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0CsmzkKpb84"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_model_and_tokenizer(hugging_face_path):\n",
        "    model = transformers.AutoModelForSequenceClassification.from_pretrained(hugging_face_path)\n",
        "    model = model.to(device)\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(hugging_face_path)\n",
        "    model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
        "    return model_wrapper\n",
        "\n",
        "\n",
        "def get_use_embeddings(use, succesful_attacks):\n",
        "    original_texts = [res.original_text() for res in succesful_attacks]\n",
        "    adversarial_texts = [res.perturbed_text() for res in succesful_attacks]\n",
        "    use_original_embeddings = use.encode(original_texts)\n",
        "    use_adv_embeddings = use.encode(adversarial_texts)\n",
        "    return use_original_embeddings, use_adv_embeddings\n",
        "\n",
        "\n",
        "def get_avg_cosine_sim(original_embeddings, adv_embeddings):\n",
        "    # Compute avg. cosine similarity between the embeddings computed using the Universal Sentence Encoding\n",
        "    cos_sim = []\n",
        "    sim_metric = torch.nn.CosineSimilarity(dim=1)\n",
        "    cosine_similarities = sim_metric(torch.tensor(original_embeddings),\n",
        "                                     torch.tensor(adv_embeddings))\n",
        "    return np.mean(cosine_similarities.numpy())\n",
        "\n",
        "\n",
        "\n",
        "def get_cosine_sim(original_embeddings, adv_embeddings):\n",
        "    cos_sim = []\n",
        "    sim_metric = torch.nn.CosineSimilarity(dim=1)\n",
        "    cosine_similarities = sim_metric(torch.tensor(original_embeddings),\n",
        "                                     torch.tensor(adv_embeddings))\n",
        "    return cosine_similarities.numpy().tolist()\n",
        "\n",
        "\n",
        "def get_words_embedding_distances(embedding, original_text,\n",
        "                                  adv_text):\n",
        "    word_distances = []\n",
        "    for word, adv_word in zip(original_text, adv_text):\n",
        "        if word != adv_word:\n",
        "            # print(word, adv_word)\n",
        "            try:\n",
        "              cos_sim = embedding.get_cos_sim(word, adv_word)\n",
        "            except KeyError:\n",
        "              continue\n",
        "            word_distances.append( (word, adv_word, cos_sim) )\n",
        "\n",
        "    return word_distances\n",
        "\n",
        "\n",
        "def get_min_words_distance(embedding, succesful_attacks):\n",
        "    min_distances = []\n",
        "    for res in succesful_attacks:\n",
        "        original_text = res.original_result.attacked_text.words\n",
        "        adversarial_text = res.perturbed_result.attacked_text.words\n",
        "        if len(original_text) != len(adversarial_text):\n",
        "            continue\n",
        "        embedding_distances = get_words_embedding_distances(embedding,\n",
        "                                                            original_text,\n",
        "                                                            adversarial_text)\n",
        "        # Get the words with minimum embedding cosine similarity\n",
        "        if len(embedding_distances) == 0:\n",
        "            continue\n",
        "        min_words_distance_i = min(embedding_distances, key = lambda x: x[2])\n",
        "        min_distances.append(min_words_distance_i)\n",
        "    return min_distances\n",
        "\n",
        "def display_results(attacks_results, output_file):\n",
        "    pd.options.display.max_colwidth = (\n",
        "    480  # increase colum width so we can actually read the examples\n",
        "    )\n",
        "\n",
        "    logger = CSVLogger(color_method=\"html\")\n",
        "\n",
        "    for result in attacks_results:\n",
        "        if isinstance(result, SuccessfulAttackResult):\n",
        "            logger.log_attack_result(result)\n",
        "\n",
        "    results = pd.DataFrame.from_records(logger.row_list)\n",
        "\n",
        "    if not results.empty:\n",
        "        html_content = results[[\"original_text\", \"perturbed_text\"]].to_html(escape=False)\n",
        "\n",
        "        with open(output_file, 'w') as file:\n",
        "            file.write(html_content)\n",
        "\n",
        "\n",
        "def transfer_attack(model_wrapper, attack_results):\n",
        "    original_text = [res.original_text() for res in attack_results]\n",
        "    adversarial_text = [res.perturbed_text() for res in attack_results]\n",
        "    original_labels = [res.original_result.ground_truth_output for res in attack_results]\n",
        "    original_model_predicted_labels = np.argmax(model_wrapper(original_text), axis=1)\n",
        "    adv_model_pred_labels = np.argmax(model_wrapper(adversarial_text), axis=1)\n",
        "\n",
        "    original_accuracy = accuracy_score(original_labels,\n",
        "                                       original_model_predicted_labels)\n",
        "    adv_accuracy = accuracy_score(original_labels, adv_model_pred_labels)\n",
        "    return original_accuracy, adv_accuracy\n",
        "\n",
        "\n",
        "def create_results_dict(original_embeddings, adversarial_embeddings, cosine_sim,\n",
        "                    perturbed_embedding_distances):\n",
        "    use_data_dic = {}\n",
        "    use_data_dic['original_embeddings'] = original_embeddings\n",
        "    use_data_dic['adversarial_embeddings'] = adversarial_embeddings\n",
        "    use_data_dic['cosine_sim'] = cosine_sim\n",
        "    use_data_dic['perturbed_embedding_distances'] = perturbed_embedding_distances\n",
        "    return use_data_dic\n",
        "\n",
        "def dictToJSON(data, filename = \"data.json\"):\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "\n",
        "def perform_attack(adv_alg, model, dataset, file_name, parallel = True,\n",
        "                   num_example = -1):\n",
        "    attack = adv_alg.build(model)\n",
        "    attack_args = textattack.AttackArgs(num_examples=num_example,\n",
        "                                        parallel = parallel,\n",
        "                                        disable_stdout = True)\n",
        "    attacker = Attacker(attack, dataset, attack_args)\n",
        "    adv_results = attacker.attack_dataset()\n",
        "\n",
        "    display_results(adv_results, file_name)\n",
        "\n",
        "    succesful_attacks = list(filter(lambda res: isinstance(res, SuccessfulAttackResult),\n",
        "                          adv_results))\n",
        "    results_dict = {}\n",
        "    if len(succesful_attacks) != 0:\n",
        "        original_embeddings, adv_embeddings = get_use_embeddings(use,\n",
        "                                                            succesful_attacks)\n",
        "\n",
        "        avg_use_cosine_sim = get_avg_cosine_sim(original_embeddings,adv_embeddings)\n",
        "        print(f\"Average cosine similarity {avg_use_cosine_sim}\")\n",
        "\n",
        "        cosine_sim = get_cosine_sim(original_embeddings,adv_embeddings)\n",
        "\n",
        "        min_embedding_distances = get_min_words_distance(word_embedding,\n",
        "                                                 succesful_attacks)\n",
        "        perturbed_embedding_distances = [x[2] for x in min_embedding_distances]\n",
        "\n",
        "        for i,med in enumerate(min_embedding_distances):\n",
        "            print(f\"Adv example: {i}\")\n",
        "            print(f\"Original word: {med[0]}\")\n",
        "            print(f\"Perturbed word: {med[1]}\")\n",
        "            print(f\"Minimum Cosine similarity among words: {med[2]}\")\n",
        "            print()\n",
        "\n",
        "        results_dict = create_results_dict(original_embeddings.tolist(),\n",
        "                                      adv_embeddings.tolist(),\n",
        "                                      cosine_sim,\n",
        "                                      perturbed_embedding_distances)\n",
        "\n",
        "        return results_dict, adv_results\n",
        "\n",
        "\n",
        "\n",
        "class FixedHuggingFaceDataset(HuggingFaceDataset):\n",
        "    def __init__(self, name_or_dataset, subset=None, split=\"train\", dataset_columns=None, label_map=None,\n",
        "                 label_names=None, output_scale_factor=None, shuffle=False, seed=69, subset_size=None, offset=0):\n",
        "        super().__init__(name_or_dataset=name_or_dataset, subset=subset, split=split, dataset_columns=dataset_columns,\n",
        "                         label_map=label_map, label_names=label_names, output_scale_factor=output_scale_factor,\n",
        "                         shuffle=shuffle)\n",
        "        if shuffle:\n",
        "            self._dataset = self._dataset.shuffle(seed=seed).flatten_indices()\n",
        "        if subset_size is not None:\n",
        "            self._dataset = self._dataset.skip(offset).take(subset_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Af3yFNko45"
      },
      "outputs": [],
      "source": [
        "use = UniversalSentenceEncoder()\n",
        "word_embedding = WordEmbedding.counterfitted_GLOVE_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tl2edDh6WGC"
      },
      "outputs": [],
      "source": [
        "models = {}\n",
        "datasets = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnRg_I5lvVbB"
      },
      "source": [
        "## Attacked models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZUttuvAvYP7"
      },
      "outputs": [],
      "source": [
        "bert_imdb = load_model_and_tokenizer(\"textattack/bert-base-uncased-imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNr5Fmr_vYrB"
      },
      "outputs": [],
      "source": [
        "bert_yelp = load_model_and_tokenizer(\"textattack/bert-base-uncased-yelp-polarity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OKYCxUvvezM"
      },
      "outputs": [],
      "source": [
        "bert_ag = load_model_and_tokenizer(\"textattack/bert-base-uncased-ag-news\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFIPgu2g4Nwp"
      },
      "source": [
        "## Attacked models for transferability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILcGUlQq4WR8"
      },
      "outputs": [],
      "source": [
        "lstm_imdb = LSTMForClassification.from_pretrained(\"lstm-imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXy_S-mD4mF-"
      },
      "outputs": [],
      "source": [
        "tokenizer = lstm_imdb.tokenizer\n",
        "lstm_imdb_wrapper = PyTorchModelWrapper(lstm_imdb, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYz-pcKcVHUj"
      },
      "outputs": [],
      "source": [
        "cnn_imdb = WordCNNForClassification.from_pretrained(\"cnn-imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efHyozvrVHwm"
      },
      "outputs": [],
      "source": [
        "tokenizer = cnn_imdb.tokenizer\n",
        "cnn_imdb_wrapper = PyTorchModelWrapper(cnn_imdb, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dxkBbZqvi_B"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHyYvDVOvm9I"
      },
      "outputs": [],
      "source": [
        "imdb = FixedHuggingFaceDataset(\"imdb\", split=\"test\", subset_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DK2Jm6svsao"
      },
      "outputs": [],
      "source": [
        "yelp_polarity = FixedHuggingFaceDataset(\"yelp_polarity\", split=\"test\", subset_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJUFd8FUvuOA"
      },
      "outputs": [],
      "source": [
        "ag_news = FixedHuggingFaceDataset(\"ag_news\", split=\"test\", subset_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3exRhjfBcWp"
      },
      "source": [
        "## BAE: BERT-based Adversarial Examples for Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ3X4tQPsDyc"
      },
      "outputs": [],
      "source": [
        "from textattack import Attacker\n",
        "from textattack.attack_recipes import BAEGarg2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwFBvl1ysL9N"
      },
      "source": [
        "### IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD_i2kGxdNwV"
      },
      "outputs": [],
      "source": [
        "results_bae_imdb, attack_results = perform_attack(BAEGarg2019,\n",
        "                                                     bert_imdb,\n",
        "                                                     imdb,\n",
        "                                                     \"bae_results_imdb.html\")\n",
        "datasets['imdb'] = results_bae_imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5caGTokqYYbh"
      },
      "outputs": [],
      "source": [
        "#Transferability\n",
        "bae_original_acc_lstm, bae_adv_acc_lstm = transfer_attack(lstm_imdb_wrapper,\n",
        "                                                          attack_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6SgVWt4ZGvI"
      },
      "outputs": [],
      "source": [
        "print(f\"Original accuracy lstm: {bae_original_acc_lstm}\")\n",
        "print(f\"Accuracy after the attack lstm: {bae_adv_acc_lstm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxt5PpxgVYei"
      },
      "outputs": [],
      "source": [
        "bae_original_acc_cnn, bae_adv_acc_cnn = transfer_attack(cnn_imdb_wrapper,\n",
        "                                                          attack_results)\n",
        "print(f\"Original accuracy cnn: {bae_original_acc_cnn}\")\n",
        "print(f\"Accuracy after the attack cnn: {bae_adv_acc_cnn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nuhGu7AX80f"
      },
      "source": [
        "### Yelp polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DNtm4ECejaP"
      },
      "outputs": [],
      "source": [
        "results_bae_yelp, _ = perform_attack(BAEGarg2019,\n",
        "                                     bert_yelp,\n",
        "                                     yelp_polarity,\n",
        "                                     \"bae_results_yelp.html\")\n",
        "datasets['yelp'] = results_bae_yelp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qREPgg60wNhu"
      },
      "source": [
        "### AG news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggKV9pl-fbKU"
      },
      "outputs": [],
      "source": [
        "results_bae_ag, _ = perform_attack(BAEGarg2019,\n",
        "                                     bert_ag,\n",
        "                                     ag_news,\n",
        "                                     \"bae_results_ag.html\")\n",
        "datasets['ag_news'] = results_bae_ag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdD1hVMb650k"
      },
      "outputs": [],
      "source": [
        "models['BAE'] = datasets.copy()\n",
        "datasets.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OklnKb3p_W1"
      },
      "source": [
        "## BESA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWv5rxyeqand"
      },
      "outputs": [],
      "source": [
        "from textattack.search_methods import SearchMethod\n",
        "from textattack.goal_function_results import GoalFunctionResultStatus\n",
        "# from textattack.constraints.pre_transformation import RepeatModification\n",
        "from textattack.shared.validators import transformation_consists_of_word_swaps\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGOs91FeqTxL"
      },
      "outputs": [],
      "source": [
        "class SimulatedAnnealing(SearchMethod):\n",
        "\n",
        "    def __init__(self, max_temp=1000.0, K=10, radius=3, delta=0.01,\n",
        "                 min_temp = 50.0):\n",
        "        self.max_temp = max_temp\n",
        "        self.K = K\n",
        "        self.radius = radius\n",
        "        self.delta = delta\n",
        "        self.min_temp = min_temp\n",
        "\n",
        "    def _get_score(self, original_text, adversarial_text):\n",
        "        result, search_over = self.get_goal_results([adversarial_text])\n",
        "        score = result[0].score\n",
        "        cost = len(original_text.all_words_diff(adversarial_text))\n",
        "        y = score - (self.delta * cost)\n",
        "        return y, search_over, result[0]\n",
        "\n",
        "\n",
        "    def _get_transformations(self, indices, current_text, initial_result):\n",
        "        transformations = []\n",
        "        scores = []\n",
        "        curr_result = initial_result\n",
        "        for i in indices:\n",
        "            t_i = self.get_transformations(\n",
        "                    current_text,\n",
        "                    original_text=initial_result.attacked_text,\n",
        "                    indices_to_modify=[i])\n",
        "            if len(t_i) == 0:\n",
        "                transformations.append(current_text.words[i])\n",
        "                scores.append(curr_result.score)\n",
        "                continue\n",
        "\n",
        "            res_i, _ = self.get_goal_results(t_i)\n",
        "            scores_i = np.array([r.score for r in res_i])\n",
        "            max_score_index = np.argmax(scores_i)\n",
        "            curr_result = res_i[max_score_index]\n",
        "            scores.append(scores_i[max_score_index])\n",
        "            assert len(t_i[max_score_index].words) == len(current_text.words)\n",
        "            transformations.append(t_i[max_score_index].words[i])\n",
        "        return np.array(transformations), np.array(scores)\n",
        "\n",
        "    def perform_search(self, initial_result):\n",
        "        best_result = initial_result\n",
        "        current_text = initial_result.attacked_text\n",
        "        max_temp = self.max_temp\n",
        "        t = 0\n",
        "\n",
        "        _, indices = self.get_indices_to_order(current_text)\n",
        "        transformations, scores = self._get_transformations(indices, current_text, initial_result)\n",
        "        transformations = list(transformations[(-scores).argsort()])\n",
        "        indices = np.array(indices)[(-scores).argsort()]\n",
        "        n = len(transformations)\n",
        "        if n == 0:\n",
        "            return best_result\n",
        "\n",
        "        while not best_result.goal_status == GoalFunctionResultStatus.SUCCEEDED:\n",
        "            if max_temp < self.min_temp:\n",
        "                return best_result\n",
        "\n",
        "            #Internal simulations\n",
        "            for k in range(self.K):\n",
        "                index = t + random.randint(0, t*self.radius)\n",
        "                if index >= n:\n",
        "                    continue\n",
        "                original_index = indices[index]\n",
        "                word_adv = transformations[index]\n",
        "                next_text = current_text.replace_word_at_index(original_index, word_adv)\n",
        "\n",
        "                curr_score = best_result.score\n",
        "                next_score, search_over, result = self._get_score(initial_result.attacked_text, next_text)\n",
        "                if search_over:\n",
        "                    return result\n",
        "                delta = next_score - curr_score\n",
        "                if (delta > 0) or (random.random() < np.exp(delta / max_temp)):\n",
        "                    current_text = next_text\n",
        "                    best_result = result\n",
        "            t += 1\n",
        "            #Annealing\n",
        "            max_temp = max_temp / (t + 1)\n",
        "\n",
        "        return best_result\n",
        "\n",
        "    def check_transformation_compatibility(self, transformation):\n",
        "        \"\"\"The SA algorithm is specifically designed for word\n",
        "        substitutions.\"\"\"\n",
        "        return transformation_consists_of_word_swaps(transformation)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def is_black_box(self):\n",
        "        return True\n",
        "\n",
        "    def extra_repr_keys(self):\n",
        "        return [\"max_temp\", \"K\", \"radius\", \"delta\", \"min_temp\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWNw6gCO2M9H"
      },
      "outputs": [],
      "source": [
        "from textattack.attack_recipes.attack_recipe import AttackRecipe\n",
        "from textattack.transformations import WordSwapMaskedLM\n",
        "from textattack.goal_functions import UntargetedClassification\n",
        "from textattack.constraints.grammaticality import PartOfSpeech\n",
        "from textattack import Attack\n",
        "# from textattack.constraints.pre_transformation import StopwordModification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONU23ZZYyjN6"
      },
      "outputs": [],
      "source": [
        "class BESAYang2020(AttackRecipe):\n",
        "\n",
        "    @staticmethod\n",
        "    def build(model_wrapper):\n",
        "        transformation = WordSwapMaskedLM(\n",
        "            method=\"bae\", max_candidates=50\n",
        "        )\n",
        "        # constraints = [StopwordModification()]\n",
        "        constraints = []\n",
        "        constraints.append(PartOfSpeech())\n",
        "        use_constraint = UniversalSentenceEncoder(\n",
        "            threshold=0.5,\n",
        "            metric=\"cosine\",\n",
        "            window_size = 15\n",
        "        )\n",
        "        constraints.append(use_constraint)\n",
        "\n",
        "        goal_function = UntargetedClassification(model_wrapper)\n",
        "        search_method = SimulatedAnnealing()\n",
        "\n",
        "        return BESAYang2020(goal_function, constraints, transformation, search_method)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udAeU2CFy7su"
      },
      "source": [
        "### IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZcR7P6IjpG0"
      },
      "outputs": [],
      "source": [
        "results_besa_imdb, attack_results = perform_attack(BESAYang2020,\n",
        "                                                     bert_imdb,\n",
        "                                                     imdb,\n",
        "                                                     \"besa_results_imdb.html\",\n",
        "                                                      parallel=False)\n",
        "datasets['imdb'] = results_besa_imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjvfMoNWjzDT"
      },
      "outputs": [],
      "source": [
        "#Transferability\n",
        "besa_original_acc_lstm, besa_adv_acc_lstm = transfer_attack(lstm_imdb_wrapper,\n",
        "                                                          attack_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlp2XyOIj7ZI"
      },
      "outputs": [],
      "source": [
        "print(f\"Original accuracy lstm: {besa_original_acc_lstm}\")\n",
        "print(f\"Accuracy after the attack lstm: {besa_adv_acc_lstm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb-w-m4TVopO"
      },
      "outputs": [],
      "source": [
        "besa_original_acc_cnn, besa_adv_acc_cnn = transfer_attack(cnn_imdb_wrapper,\n",
        "                                                          attack_results)\n",
        "print(f\"Original accuracy cnn: {besa_original_acc_cnn}\")\n",
        "print(f\"Accuracy after the attack cnn: {besa_adv_acc_cnn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU684wjqzGXm"
      },
      "source": [
        "### Yelp polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_WhpVGNkA_J"
      },
      "outputs": [],
      "source": [
        "results_besa_yelp, _ = perform_attack(BESAYang2020,\n",
        "                                      bert_yelp,\n",
        "                                      yelp_polarity,\n",
        "                                      \"besa_results_yelp.html\",\n",
        "                                      parallel=False)\n",
        "datasets['yelp'] = results_besa_yelp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npvrWpsP_ufN"
      },
      "source": [
        "### AG news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD81RCENkVO-"
      },
      "outputs": [],
      "source": [
        "results_besa_ag, _ = perform_attack(BESAYang2020,\n",
        "                                    bert_ag,\n",
        "                                    ag_news,\n",
        "                                    \"besa_results_ag.html\",\n",
        "                                    parallel = False)\n",
        "datasets['ag_news'] = results_besa_ag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arqH8st4-KBm"
      },
      "outputs": [],
      "source": [
        "models['BESA'] = datasets.copy()\n",
        "datasets.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeOc5lbTsjgx"
      },
      "source": [
        "## Charts / Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5-AY9X0BDxv"
      },
      "outputs": [],
      "source": [
        "box_plots_data = []\n",
        "model_labels = []\n",
        "\n",
        "for model in models.keys():\n",
        "    cosine_similarities = []\n",
        "    datasets_i = models[model]\n",
        "    model_labels.append(model)\n",
        "    for dataset in datasets_i.keys():\n",
        "        dataset_dict = datasets_i[dataset]\n",
        "        if dataset_dict is not None:\n",
        "            cosine_similarities +=  dataset_dict['cosine_sim']\n",
        "    box_plots_data.append(cosine_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy8x_LKj8_8S"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure and an axis\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Create the box plots\n",
        "ax.boxplot(box_plots_data)\n",
        "\n",
        "# Set the labels for the x-axis\n",
        "ax.set_xticklabels(model_labels)\n",
        "\n",
        "# Add title and labels\n",
        "ax.set_title('USE score')\n",
        "ax.set_xlabel('Adversarial algorithms')\n",
        "ax.set_ylabel('Use scores')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efDQ55P6nKtW"
      },
      "source": [
        "## DeepWordBug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkEEBZZingtl"
      },
      "outputs": [],
      "source": [
        "from textattack.attack_recipes import DeepWordBugGao2018"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPhSoAaCwxye"
      },
      "source": [
        "### IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwEtVwnLhZ32"
      },
      "outputs": [],
      "source": [
        "results_dwb_imdb, attack_results = perform_attack(DeepWordBugGao2018,\n",
        "                                                     bert_imdb,\n",
        "                                                     imdb,\n",
        "                                                     \"dwb_results_imdb.html\")\n",
        "datasets['imdb'] = results_dwb_imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSC1NOMDdT05"
      },
      "outputs": [],
      "source": [
        "#Transferability\n",
        "dwb_original_acc_lstm, dwb_adv_acc_lstm = transfer_attack(lstm_imdb_wrapper,\n",
        "                                                          attack_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84aHkQbKdfNQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Original accuracy lstm: {dwb_original_acc_lstm}\")\n",
        "print(f\"Accuracy after the attack lstm: {dwb_adv_acc_lstm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyGrGbI0ViMd"
      },
      "outputs": [],
      "source": [
        "dwb_original_acc_cnn, dwb_adv_acc_cnn = transfer_attack(cnn_imdb_wrapper,\n",
        "                                                          attack_results)\n",
        "print(f\"Original accuracy cnn: {dwb_original_acc_cnn}\")\n",
        "print(f\"Accuracy after the attack cnn: {dwb_adv_acc_cnn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH28ASchw0b-"
      },
      "source": [
        "### Yelp polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szo9jmBMi7IJ"
      },
      "outputs": [],
      "source": [
        "results_dwb_yelp, _ = perform_attack(DeepWordBugGao2018,\n",
        "                                     bert_yelp,\n",
        "                                     yelp_polarity,\n",
        "                                     \"dwb_results_yelp.html\")\n",
        "datasets['yelp'] = results_dwb_yelp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci3laKvJx2zn"
      },
      "source": [
        "### AG news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSTq5OAxjIUR"
      },
      "outputs": [],
      "source": [
        "results_dwb_ag, _ = perform_attack(DeepWordBugGao2018,\n",
        "                                     bert_ag,\n",
        "                                     ag_news,\n",
        "                                     \"dwb_results_ag.html\")\n",
        "datasets['ag_news'] = results_dwb_ag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_CtxmBU9f6p"
      },
      "outputs": [],
      "source": [
        "models['DWB'] = datasets.copy()\n",
        "datasets.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtdPObUkWJ4A"
      },
      "outputs": [],
      "source": [
        "dictToJSON(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A7Kkl0F2CHP"
      },
      "source": [
        "## New attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mlU-qM1VuH8"
      },
      "outputs": [],
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "class WeightedSimulatedAnnealing(SearchMethod):\n",
        "\n",
        "    def __init__(self, max_temp=1000.0, K=10, radius=3, delta=0.01,\n",
        "                 min_temp = 50.0):\n",
        "        self.max_temp = max_temp\n",
        "        self.K = K\n",
        "        self.radius = radius\n",
        "        self.delta = delta\n",
        "        self.use = UniversalSentenceEncoder()\n",
        "        self.use_penalty_weight = 0.9\n",
        "        self.min_temp = min_temp\n",
        "\n",
        "    def _cosine_similarity(self, embedding1, embedding2):\n",
        "        return dot(embedding1, embedding2)/(norm(embedding1)*norm(embedding2))\n",
        "\n",
        "\n",
        "    def _get_use_similarity(self, text1, text2):\n",
        "        embeddings = self.use.encode([text1, text2])\n",
        "        similarity = self._cosine_similarity(embeddings[0], embeddings[1])\n",
        "        return similarity\n",
        "\n",
        "\n",
        "    def _get_score(self, original_text, adversarial_text):\n",
        "        result, search_over = self.get_goal_results([adversarial_text])\n",
        "        score = result[0].score\n",
        "        # cost = len(original_text.all_words_diff(adversarial_text))\n",
        "        # y = score - (self.delta * cost)\n",
        "        use_similarity = self._get_use_similarity(original_text.text,\n",
        "                                                  adversarial_text.text)\n",
        "        use_penalty = self.use_penalty_weight * (1 - use_similarity)\n",
        "        y = score - use_penalty\n",
        "        return y, search_over, result[0]\n",
        "\n",
        "\n",
        "    def _get_transformations(self, indices, current_text, initial_result):\n",
        "        transformations = []\n",
        "        scores = []\n",
        "        curr_result = initial_result\n",
        "        for i in indices:\n",
        "            t_i = self.get_transformations(\n",
        "                    current_text,\n",
        "                    original_text=initial_result.attacked_text,\n",
        "                    indices_to_modify=[i])\n",
        "            if len(t_i) == 0:\n",
        "                transformations.append(current_text.words[i])\n",
        "                scores.append(curr_result.score)\n",
        "                continue\n",
        "\n",
        "            res_i, _ = self.get_goal_results(t_i)\n",
        "            scores_i = np.array([r.score for r in res_i])\n",
        "            max_score_index = np.argmax(scores_i)\n",
        "            curr_result = res_i[max_score_index]\n",
        "            scores.append(scores_i[max_score_index])\n",
        "            assert len(t_i[max_score_index].words) == len(current_text.words)\n",
        "            transformations.append(t_i[max_score_index].words[i])\n",
        "        return np.array(transformations), np.array(scores)\n",
        "\n",
        "    def perform_search(self, initial_result):\n",
        "        best_result = initial_result\n",
        "        current_text = initial_result.attacked_text\n",
        "        max_temp = self.max_temp\n",
        "        t = 0\n",
        "\n",
        "        _, indices = self.get_indices_to_order(current_text)\n",
        "        transformations, scores = self._get_transformations(indices, current_text, initial_result)\n",
        "        transformations = list(transformations[(-scores).argsort()])\n",
        "        indices = np.array(indices)[(-scores).argsort()]\n",
        "        n = len(transformations)\n",
        "        if n == 0:\n",
        "            return best_result\n",
        "\n",
        "        while not best_result.goal_status == GoalFunctionResultStatus.SUCCEEDED:\n",
        "            if max_temp < self.min_temp:\n",
        "                return best_result\n",
        "\n",
        "            #Internal simulations\n",
        "            for k in range(self.K):\n",
        "                index = t + random.randint(0, t*self.radius)\n",
        "                if index >= n:\n",
        "                    continue\n",
        "                original_index = indices[index]\n",
        "                word_adv = transformations[index]\n",
        "                next_text = current_text.replace_word_at_index(original_index, word_adv)\n",
        "\n",
        "                curr_score = best_result.score\n",
        "                next_score, search_over, result = self._get_score(initial_result.attacked_text, next_text)\n",
        "                if search_over:\n",
        "                    return result\n",
        "                delta = next_score - curr_score\n",
        "                if (delta > 0) or (random.random() < np.exp(delta / max_temp)):\n",
        "                    current_text = next_text\n",
        "                    best_result = result\n",
        "            t += 1\n",
        "            #Annealing\n",
        "            max_temp = max_temp / (t + 1)\n",
        "\n",
        "        return best_result\n",
        "\n",
        "    def check_transformation_compatibility(self, transformation):\n",
        "        \"\"\"The SA algorithm is specifically designed for word\n",
        "        substitutions.\"\"\"\n",
        "        return transformation_consists_of_word_swaps(transformation)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def is_black_box(self):\n",
        "        return True\n",
        "\n",
        "    def extra_repr_keys(self):\n",
        "        return [\"max_temp\", \"K\", \"radius\", \"delta\", \"min_temp\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o07HlAZDV1F2"
      },
      "outputs": [],
      "source": [
        "class OurAttack(AttackRecipe):\n",
        "\n",
        "    @staticmethod\n",
        "    def build(model_wrapper):\n",
        "        transformation = WordSwapMaskedLM(\n",
        "            method=\"bae\", max_candidates=50\n",
        "        )\n",
        "        # constraints = [StopwordModification()]\n",
        "        constraints = []\n",
        "        constraints.append(PartOfSpeech())\n",
        "        use_constraint = UniversalSentenceEncoder(\n",
        "            threshold=0.93,\n",
        "            metric=\"cosine\",\n",
        "            window_size = 15\n",
        "        )\n",
        "        constraints.append(use_constraint)\n",
        "\n",
        "        goal_function = UntargetedClassification(model_wrapper)\n",
        "        search_method = WeightedSimulatedAnnealing()\n",
        "\n",
        "        return OurAttack(goal_function, constraints, transformation, search_method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMTcGciDV3-G"
      },
      "outputs": [],
      "source": [
        "results_our_yelp, attack_results = perform_attack(OurAttack,\n",
        "                                                     bert_yelp,\n",
        "                                                     yelp_polarity,\n",
        "                                                     \"our_results_yelp.html\",\n",
        "                                                      parallel=False,\n",
        "                                                      num_example=32)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "bwFBvl1ysL9N",
        "-nuhGu7AX80f",
        "qREPgg60wNhu",
        "LU684wjqzGXm",
        "npvrWpsP_ufN",
        "OeOc5lbTsjgx",
        "rPhSoAaCwxye",
        "JH28ASchw0b-",
        "ci3laKvJx2zn",
        "2A7Kkl0F2CHP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}